# -*- coding: utf-8 -*-
"""Copy of MLproject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j0rRcNFQ-KAYfmol2-UiLSVOYunNpa5y
"""

from google.colab import files
uploaded = files.upload()
import pandas as pd
import io
housing = pd.read_csv(io.BytesIO(uploaded['housingdata.csv']))

#to print the top five rows of the data we make use of the .head() method
print(housing.head())
#to verify that none of the entries is Null we make use of the .info() method
print(housing.info())
#to get a count of each of the values we make use of the .value_counts() method
print(housing['AGE'].value_counts())
#we can draw a histogram of all the features with the help of matplotlib 
import matplotlib.pyplot as plt
housing.hist(bins=50,figsize=(20,15))
plt.show()

import numpy as np
def split_train_test(data,test_ratio):
  np.random.seed(42)
  shuffled=np.random.permutation(len(data))
  test_size=int(len(data)*test_ratio)
  test_indices=shuffled[:test_size]
  train_indices=shuffled[test_size:]
  return data.iloc[test_indices],data.iloc[train_indices]
test_size,train_size=split_train_test(housing,0.2)
print("Rows in test size:",len(test_size))
print("Rows in train size:",len(train_size))

#What we had earlier done manually can also be done using scikitlearn
import sklearn
from sklearn.model_selection import train_test_split
train_set,test_set=train_test_split(housing,test_size=0.2,random_state=42)
print("Rows in test size:",len(test_size))
print("Rows in train size:",len(train_size))

#Now the problem arises that in the 'CHAS' part we have 357 entries which contain 0 and some 100 which contain 1 entry
#suppose if our training set only consists of the values 0 then when we make use of the test data to find the accuracy we will get a high degree of error
#because it would never have seen any data with 1
#for this we have to stratigically choose our data
from sklearn.model_selection import StratifiedShuffleSplit
split=StratifiedShuffleSplit()
for train_index,test_index in split.split(housing,housing['CHAS']):
  strat_test_set=housing.loc[test_index]
  strat_train_set=housing.loc[train_index]
print(strat_test_set['CHAS'].value_counts())
print(strat_train_set['CHAS'].value_counts())

housing=strat_train_set.copy()
print(housing.describe())

"""Looking for Correlations"""

#Strong Pierssen Correlation
#tells us the relation between the quantities and the values measured
#finds the relation between the different columns in the data 
corr_matrix=housing.corr()
corr_matrix['MEDV'].sort_values()

"""*Attributes*"""

from pandas.plotting import scatter_matrix
attributes=['LSTAT','TAX','RM','MEDV']
scatter_matrix(housing[attributes],figsize=(10,10),alpha=0.8)

housing.plot(kind="scatter",x='RM',y='MEDV',alpha=0.8)
housing.plot(kind="scatter",x='LSTAT',y='MEDV',alpha=0.8)

"""Attribute Combinations

"""

#we can also make our own attributes
#example we can make rooms/tax
housing["TAXRM"]=housing["RM"]/housing["TAX"]
print(housing["TAXRM"])
print(housing.head())
mat=housing.corr()
print(mat['MEDV'].sort_values(ascending=False))
housing.plot(kind="scatter",x="TAXRM",y="MEDV")

#we are seperating the features and labels from the dataset
housing=strat_train_set.drop(["MEDV"],axis=1)
# print(housing.describe())
housing_label=strat_train_set["MEDV"].copy()
print(housing_label)

"""**Missing** **Attributes**"""

#when we have missing attributes in your datasets then we can deal with them in three different ways:
#1.Remove the missing datasets
#2.Remove the entire attribute/Column
#3.Replace the missing values wiht the median/mode etc
#METHOD 1:
a=housing.dropna(axis=0,subset=["LSTAT"])
print(a.shape)
#Note that this changes are being made in the copy of the original dataset
#METHOD 2:
b=housing.drop("LSTAT",axis=1)
print(b.shape)
#METHOD 3:
median=housing["LSTAT"].median()
print(median)
c=housing["LSTAT"].fillna(median)
# print(c)
mean=housing["LSTAT"].mean()
print(mean)
d=housing["LSTAT"].fillna(mean)
print(d)

#We can make use of the scikit learn to accomplish the above objectives
import sklearn
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer
print(housing.describe())
imputer=SimpleImputer(strategy="mean")  
imputer=imputer.fit(housing)
X=imputer.transform(housing)
housing_tr=pd.DataFrame(X,columns=housing.columns)
print(housing_tr.describe())

"""Scikitlearn-Design
It primarily consists of three objects
1.Estimators:
It estimates some parameter based on a dataset E.g imputer
it has a fit and a transform method 
Fit method:Fits the dataset and calculates the internal parameters
2.Transformers:
transform method takes input and returns output based on the learnings from fit().It also has a convenience function called called fit_transform() which fits and then transforms
3.Predictors:
LinearRegression model is an exmple of Predictor.fit() and predict() are two common functions.It also gives score() function which will evaluate the predictions

Feature scaling:
"""

# Primarily two types of feature scaling can be done
# 1.MinMaxScaling(Normalization)
# (value-min)/(max-min)
# 2.Standardization:
# (value-mean)/standard-deviation

"""Creating a Pipeline:"""

import sklearn 
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
my_pipeline=Pipeline([
    ('imputer',SimpleImputer(strategy="median")),
    #Many more such can be added
    ('std_scaler',StandardScaler()),
])
housing_num_tr=my_pipeline.fit_transform(housing)
print(housing_num_tr)

"""Fitting a desired model for Solving the Problem"""

from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
# model=LinearRegression()
# model=DecisionTreeRegressor()
model=RandomForestRegressor()
model.fit(housing_num_tr,housing_label)
some_data=housing.iloc[:5]
# print(some_data)
some_label=housing_label.iloc[:5]
# print(some_label)
prepared_data=my_pipeline.transform(housing_num_tr)
prediction=model.predict(prepared_data)

#Find the root mean square error
from sklearn.metrics import mean_squared_error
print("Root mean squared error is:",np.sqrt(mean_squared_error(housing_label,prediction)))

"""Better evaluating techniques-Cross Validation

"""

from sklearn.model_selection import cross_val_score
values=cross_val_score(model,housing_num_tr,housing_label,scoring="neg_mean_squared_error",cv=10)
rmse=np.sqrt(-values)
print(rmse)

from joblib import load,dump
dump(model,"pricepredictor.joblib")

X_test=strat_test_set.drop(["MEDV"],axis=1)
Y_test=strat_test_set["MEDV"]
prepared_data=my_pipeline.transform(X_test)
Y_predicted=model.predict(prepared_data)
mse=mean_squared_error(Y_predicted,Y_test)
rmse=np.sqrt(mse)
print("Root mean squared error is:", rmse)

"""Using the Model"""

from joblib import dump,load
import numpy as np
model=load('pricepredictor.joblib')
features=np.array([[0.00632	,18	,2.31,	0,	0.538,	6.575,	65.2,	4.09,	1,	296,	15.3,	396.9,9.14]])
print(model.predict(features))